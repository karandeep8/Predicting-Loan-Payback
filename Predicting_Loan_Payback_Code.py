# -*- coding: utf-8 -*-
"""Untitled49.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e9tP0jWCbpNQXtE_Z7ed_GaV_COOlGLe
"""

"""
=========================================================================================
LOAN PAYBACK PREDICTION - COMPLETE MACHINE LEARNING PIPELINE
=========================================================================================

PROJECT OVERVIEW:
-----------------
This project aims to predict whether a borrower will pay back their loan based on
various features like income, credit score, debt-to-income ratio, demographics, etc.

WHY THIS MATTERS:
- Financial institutions need to assess credit risk before approving loans
- Accurate predictions help reduce default rates and financial losses
- Machine learning can identify complex patterns that traditional methods might miss

APPROACH:
---------
We'll build a comprehensive classification pipeline that includes:
1. Exploratory Data Analysis (EDA) - Understanding our data
2. Data Preprocessing - Cleaning and preparing data for modeling
3. Feature Engineering - Creating new features to improve predictions
4. Model Building - Testing multiple algorithms with default parameters
5. Hyperparameter Tuning - Optimizing each model using cross-validation
6. Ensemble Methods - Combining models for better performance
7. Model Evaluation - Comparing all models using F1-score
8. Final Prediction - Generating predictions on test data

PRIMARY METRIC: F1-Score
------------------------
WHY F1-SCORE?
- F1-score balances precision (avoiding false positives) and recall (catching true positives)
- In loan prediction, both are important:
  * High Precision: Don't reject good borrowers (lost business)
  * High Recall: Don't approve bad borrowers (financial loss)
- F1-score is especially useful when classes are imbalanced (more paid loans than defaults)

=========================================================================================
"""

# =========================================================================================
# SECTION 1: IMPORTING REQUIRED LIBRARIES
# =========================================================================================

"""
WHY WE NEED THESE LIBRARIES:
-----------------------------
- pandas: Data manipulation and analysis (reading CSV, handling dataframes)
- numpy: Numerical computations and array operations
- matplotlib/seaborn: Data visualization to understand patterns
- sklearn: Machine learning algorithms, preprocessing, and evaluation metrics
- warnings: Suppress unnecessary warnings for cleaner output
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Preprocessing and feature engineering
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV

# Classification models
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.neural_network import MLPClassifier

# Evaluation metrics
from sklearn.metrics import (classification_report, confusion_matrix,
                             f1_score, accuracy_score, precision_score,
                             recall_score, roc_auc_score, roc_curve)

# Set random seed for reproducibility
"""
WHY SET RANDOM SEED?
- Ensures that random operations (train-test split, model initialization) are reproducible
- Anyone running this code will get the same results
- Critical for debugging and comparing results across different runs
"""
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# Set visualization style
"""
WHY SET STYLE?
- Consistent, professional-looking plots throughout the project
- Better readability with white grid background
- Larger default figure size for better visibility
"""
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 6)

print("="*80)
print("LOAN PAYBACK PREDICTION - EDUCATIONAL ML PIPELINE")
print("="*80)
print("\n All libraries imported successfully!")


# =========================================================================================
# SECTION 2: DATA LOADING AND INITIAL EXPLORATION
# =========================================================================================

"""
OBJECTIVE: Load the data and understand its structure, size, and basic characteristics

WHY THIS STEP IS CRITICAL:
- We need to know what we're working with before building models
- Identifies data types, missing values, and basic statistics
- Helps us plan preprocessing and feature engineering strategies
"""

print("\n" + "="*80)
print("SECTION 2: DATA LOADING AND INITIAL EXPLORATION")
print("="*80)

# Load the training and test datasets
"""
WHY SEPARATE TRAIN AND TEST FILES?
- Training data: Used to build and tune our models (has target variable)
- Test data: Used for final predictions (no target variable - we predict it)
- This mimics real-world scenarios where we predict on unseen data
"""
try:
    train_df = pd.read_csv('train.csv')
    test_df = pd.read_csv('test.csv')
    print("\n Data loaded successfully!")
    print(f"  - Training set shape: {train_df.shape}")
    print(f"  - Test set shape: {test_df.shape}")
except FileNotFoundError as e:
    print(f"\n Error: {e}")
    print("  Please ensure 'train.csv' and 'test.csv' are in the same directory as this script.")
    exit()

# Display basic information about the dataset
print("\n" + "-"*80)
print("DATASET OVERVIEW")
print("-"*80)

print("\nFirst 5 rows of training data:")
print(train_df.head())

print("\nDataset Information:")
print(train_df.info())

print("\nStatistical Summary of Numerical Features:")
print(train_df.describe())

print("\nTarget Variable Distribution:")
print(train_df['loan_paid_back'].value_counts())
print(f"\nClass Balance: {train_df['loan_paid_back'].value_counts(normalize=True)}")

"""
INTERPRETATION GUIDE:
---------------------
- Shape: Number of samples (rows) and features (columns)
- Data types: Numerical (int64, float64) vs Categorical (object)
- Missing values: Shown as 'Non-Null Count' - if less than total, we have missing data
- Statistical summary: Helps identify outliers and data ranges
- Target distribution: Shows if classes are balanced or imbalanced
"""


# =========================================================================================
# SECTION 3: EXPLORATORY DATA ANALYSIS (EDA)
# =========================================================================================

"""
OBJECTIVE: Visualize and understand the data to inform modeling decisions

WHY EDA IS ESSENTIAL:
- Reveals patterns, trends, and relationships in the data
- Identifies outliers and anomalies that might affect model performance
- Helps us understand which features are important for prediction
- Guides feature engineering and preprocessing decisions
"""

print("\n" + "="*80)
print("SECTION 3: EXPLORATORY DATA ANALYSIS (EDA)")
print("="*80)

# 3.1: Target Variable Distribution
"""
WHY ANALYZE TARGET DISTRIBUTION?
- Shows class imbalance (if one class dominates, models may be biased)
- Helps us decide if we need resampling techniques (SMOTE, undersampling)
- Informs our choice of evaluation metrics (F1-score is good for imbalanced data)
"""
print("\n[3.1] Analyzing Target Variable Distribution...")

plt.figure(figsize=(10, 6))
target_counts = train_df['loan_paid_back'].value_counts()
colors = ['#2ecc71', '#e74c3c']  # Green for paid, red for not paid
bars = plt.bar(target_counts.index, target_counts.values, color=colors, edgecolor='black', linewidth=2)

# Add value labels on bars
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
             f'{int(height)}\n({height/len(train_df)*100:.1f}%)',
             ha='center', va='bottom', fontsize=14, fontweight='bold')

plt.xlabel('Loan Paid Back (0 = No, 1 = Yes)', fontsize=15, fontweight='bold')
plt.ylabel('Number of Borrowers', fontsize=15, fontweight='bold')
plt.title('Distribution of Target Variable: Loan Payback Status',
          fontsize=16, fontweight='bold', pad=20)
plt.xticks([0, 1], ['Not Paid (0)', 'Paid (1)'], fontsize=15, fontweight='bold')
plt.yticks(fontsize=15, fontweight='bold')
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.savefig('01_target_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"Target distribution plot saved as '01_target_distribution.png'")


# 3.2: Numerical Features Analysis
"""
WHY ANALYZE NUMERICAL FEATURES?
- Understand the distribution of continuous variables (normal, skewed, etc.)
- Identify outliers that might need special treatment
- See which numerical features differ between paid and unpaid loans
- This information guides feature scaling and transformation decisions
"""
print("\n[3.2] Analyzing Numerical Features...")

# Select numerical columns (excluding 'id' and target)
numerical_cols = train_df.select_dtypes(include=['int64', 'float64']).columns.tolist()
numerical_cols = [col for col in numerical_cols if col not in ['id', 'loan_paid_back']]

print(f"Numerical features to analyze: {numerical_cols}")

# Create distribution plots for numerical features
fig, axes = plt.subplots(3, 3, figsize=(18, 14))
axes = axes.ravel()

for idx, col in enumerate(numerical_cols):
    if idx < len(axes):
        # Create histogram with KDE (Kernel Density Estimate)
        axes[idx].hist(train_df[col].dropna(), bins=30, edgecolor='black',
                      alpha=0.7, color='steelblue')
        axes[idx].set_xlabel(col, fontsize=13, fontweight='bold')
        axes[idx].set_ylabel('Frequency', fontsize=13, fontweight='bold')
        axes[idx].set_title(f'Distribution of {col}', fontsize=14, fontweight='bold')
        axes[idx].tick_params(axis='both', labelsize=11)
        axes[idx].grid(alpha=0.3)

# Remove empty subplots
for idx in range(len(numerical_cols), len(axes)):
    fig.delaxes(axes[idx])

plt.tight_layout()
plt.savefig('02_numerical_distributions.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"Numerical feature distributions saved as '02_numerical_distributions.png'")


# 3.3: Categorical Features Analysis
"""
WHY ANALYZE CATEGORICAL FEATURES?
- See which categories are most common in our dataset
- Understand how categorical features relate to loan payback
- Identify categories that might need to be combined or encoded differently
- Helps us choose appropriate encoding methods (Label Encoding, One-Hot Encoding)
"""
print("\n[3.3] Analyzing Categorical Features...")

# Select categorical columns
categorical_cols = train_df.select_dtypes(include=['object']).columns.tolist()
print(f"Categorical features to analyze: {categorical_cols}")

# Create count plots for categorical features
fig, axes = plt.subplots(2, 3, figsize=(18, 10))
axes = axes.ravel()

for idx, col in enumerate(categorical_cols):
    if idx < len(axes):
        # Count plot grouped by target variable
        counts = train_df.groupby([col, 'loan_paid_back']).size().unstack(fill_value=0)
        counts.plot(kind='bar', ax=axes[idx], color=['#e74c3c', '#2ecc71'],
                   edgecolor='black', linewidth=1.5)
        axes[idx].set_xlabel(col, fontsize=13, fontweight='bold')
        axes[idx].set_ylabel('Count', fontsize=13, fontweight='bold')
        axes[idx].set_title(f'{col} vs Loan Payback', fontsize=14, fontweight='bold')
        axes[idx].tick_params(axis='both', labelsize=10)
        axes[idx].legend(['Not Paid', 'Paid'], fontsize=11, loc='best')
        axes[idx].grid(alpha=0.3)
        plt.setp(axes[idx].xaxis.get_majorticklabels(), rotation=45, ha='right')

# Remove empty subplots
for idx in range(len(categorical_cols), len(axes)):
    fig.delaxes(axes[idx])

plt.tight_layout()
plt.savefig('03_categorical_distributions.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"Categorical feature distributions saved as '03_categorical_distributions.png'")


# 3.4: Correlation Analysis
"""
WHY ANALYZE CORRELATIONS?
- Identify which features are strongly related to the target variable
- Detect multicollinearity (highly correlated features that provide redundant information)
- Guide feature selection and engineering decisions
- Strong correlations indicate features that likely have high predictive power
"""
print("\n[3.4] Analyzing Feature Correlations...")

# Create a correlation matrix for numerical features
correlation_data = train_df[numerical_cols + ['loan_paid_back']].corr()

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_data, annot=True, fmt='.2f', cmap='coolwarm',
            center=0, square=True, linewidths=1, cbar_kws={"shrink": 0.8})
plt.title('Correlation Matrix: Features vs Target', fontsize=16, fontweight='bold', pad=20)
plt.xticks(fontsize=12, fontweight='bold', rotation=45, ha='right')
plt.yticks(fontsize=12, fontweight='bold', rotation=0)
plt.tight_layout()
plt.savefig('04_correlation_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"Correlation matrix saved as '04_correlation_matrix.png'")

# Print top correlated features with target
print("\nTop Features Correlated with Loan Payback:")
target_corr = correlation_data['loan_paid_back'].sort_values(ascending=False)
print(target_corr[target_corr.index != 'loan_paid_back'])

"""
INTERPRETATION:
- Values close to +1: Strong positive correlation (as feature increases, target increases)
- Values close to -1: Strong negative correlation (as feature increases, target decreases)
- Values close to 0: Weak or no correlation
- For loan prediction: We want features with correlations far from 0 (either positive or negative)
"""


# =========================================================================================
# SECTION 4: DATA PREPROCESSING
# =========================================================================================

"""
OBJECTIVE: Clean and prepare the data for machine learning models

WHY PREPROCESSING IS CRUCIAL:
- Models require clean, numerical data to work properly
- Missing values can cause errors or biased predictions
- Categorical variables need to be converted to numbers
- Feature scaling ensures all features contribute equally to the model
- Proper preprocessing can significantly improve model performance
"""

print("\n" + "="*80)
print("SECTION 4: DATA PREPROCESSING")
print("="*80)

# 4.1: Handling Missing Values
"""
WHY HANDLE MISSING VALUES?
- Most ML algorithms cannot handle missing (NaN) values
- Missing data can introduce bias if not handled properly
- Different strategies exist: drop, mean/median imputation, forward fill, etc.

STRATEGY CHOICE:
- For numerical features: Impute with median (robust to outliers)
- For categorical features: Impute with mode (most frequent value)
- We could also drop rows, but that loses valuable data
"""
print("\n[4.1] Handling Missing Values...")

# Check for missing values
missing_train = train_df.isnull().sum()
missing_test = test_df.isnull().sum()

print("\nMissing values in training data:")
print(missing_train[missing_train > 0])

print("\nMissing values in test data:")
print(missing_test[missing_test > 0])

# If there are missing values, we'll impute them
if missing_train.sum() > 0 or missing_test.sum() > 0:
    print("\n Imputing missing values...")

    # For numerical columns: fill with median
    for col in numerical_cols:
        if train_df[col].isnull().sum() > 0:
            median_value = train_df[col].median()
            train_df[col].fillna(median_value, inplace=True)
            test_df[col].fillna(median_value, inplace=True)
            print(f"  • {col}: Filled {missing_train[col]} values with median ({median_value:.2f})")

    # For categorical columns: fill with mode
    for col in categorical_cols:
        if train_df[col].isnull().sum() > 0:
            mode_value = train_df[col].mode()[0]
            train_df[col].fillna(mode_value, inplace=True)
            test_df[col].fillna(mode_value, inplace=True)
            print(f"  • {col}: Filled {missing_train[col]} values with mode ({mode_value})")

    print("Missing values handled successfully!")
else:
    print("No missing values found in the dataset!")


# 4.2: Encoding Categorical Variables
"""
WHY ENCODE CATEGORICAL VARIABLES?
- ML algorithms work with numbers, not text/categories
- We need to convert categories like 'Male'/'Female' into numerical format

ENCODING METHODS:
1. Label Encoding: Assigns each category a number (0, 1, 2, ...)
   - Good for: Ordinal features (where order matters) or tree-based models
   - Limitation: Implies ordering which may not exist

2. One-Hot Encoding: Creates binary columns for each category
   - Good for: Nominal features (no order) with linear models
   - Limitation: Increases dimensionality significantly

DECISION: Using Label Encoding here because:
- Tree-based models (Random Forest, Decision Tree) handle it well
- Keeps dimensionality manageable
- Most categorical features have natural ordering or few categories
"""
print("\n[4.2] Encoding Categorical Variables...")

# Create label encoders for each categorical column
label_encoders = {}

for col in categorical_cols:
    print(f"\n Encoding '{col}'...")

    # Show original categories
    print(f"  Original categories: {train_df[col].unique()}")

    # Initialize label encoder
    le = LabelEncoder()

    # Fit on training data and transform both train and test
    # Note: We fit only on train to avoid data leakage
    train_df[col] = le.fit_transform(train_df[col].astype(str))

    # For test data, handle unseen categories
    # Transform known categories, assign -1 to unknown ones
    test_df[col] = test_df[col].astype(str).map(
        lambda x: le.transform([x])[0] if x in le.classes_ else -1
    )

    # Store encoder for potential future use
    label_encoders[col] = le

    print(f"  Encoded mapping: {dict(zip(le.classes_, le.transform(le.classes_)))}")

print("\n All categorical variables encoded successfully!")


# 4.3: Feature Scaling
"""
WHY SCALE FEATURES?
- Features have different ranges (e.g., income: 1000-100000, credit_score: 300-850)
- Some algorithms (KNN, Neural Networks) are sensitive to feature scales
- Large-scale features can dominate small-scale features in distance calculations
- Scaling ensures all features contribute proportionally to the model

SCALING METHOD: StandardScaler (Z-score normalization)
- Transforms features to have mean=0 and standard deviation=1
- Formula: z = (x - mean) / std
- Works well for normally distributed data
- Alternative: MinMaxScaler (scales to 0-1 range)

IMPORTANT: We fit the scaler on training data only to prevent data leakage!
"""
print("\n[4.3] Scaling Numerical Features...")

# Prepare features for scaling (all columns except 'id' and target)
feature_cols = [col for col in train_df.columns if col not in ['id', 'loan_paid_back']]

# Separate features and target
X = train_df[feature_cols].copy()
y = train_df['loan_paid_back'].copy()
X_test = test_df[feature_cols].copy()

print(f"\nFeatures to scale: {len(feature_cols)} columns")
print(f"Training samples: {len(X)}")
print(f"Test samples: {len(X_test)}")

# Initialize and fit scaler on training data only
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_test_scaled = scaler.transform(X_test)

# Convert back to DataFrame for easier handling
X_scaled = pd.DataFrame(X_scaled, columns=feature_cols)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols)

print("\n Feature scaling completed successfully!")

print("\nBefore scaling (first feature sample):")
print(f"  Mean: {X[feature_cols[0]].mean():.2f}, Std: {X[feature_cols[0]].std():.2f}")
print(f"  Range: [{X[feature_cols[0]].min():.2f}, {X[feature_cols[0]].max():.2f}]")

print("\nAfter scaling (first feature sample):")
print(f"  Mean: {X_scaled[feature_cols[0]].mean():.2f}, Std: {X_scaled[feature_cols[0]].std():.2f}")
print(f"  Range: [{X_scaled[feature_cols[0]].min():.2f}, {X_scaled[feature_cols[0]].max():.2f}]")


# 4.4: Train-Validation Split
"""
WHY SPLIT THE DATA?
- We need to evaluate our models on unseen data to measure real-world performance
- Training on all data would lead to overfitting and unrealistic performance metrics
- Validation set acts as a proxy for real-world performance

SPLIT STRATEGY:
- 80% training: Used to train the model (learn patterns)
- 20% validation: Used to evaluate model performance (test generalization)
- Stratified split: Maintains same class proportion in both sets
- Random state: Ensures reproducibility
"""
print("\n[4.4] Splitting Data into Train and Validation Sets...")

X_train, X_val, y_train, y_val = train_test_split(
    X_scaled, y,
    test_size=0.2,           # 20% for validation
    random_state=RANDOM_STATE,
    stratify=y                # Maintain class balance
)

print(f"\nData split completed:")
print(f"  Training set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)")
print(f"  Validation set: {len(X_val)} samples ({len(X_val)/len(X)*100:.1f}%)")

print(f"\nClass distribution in training set:")
print(y_train.value_counts(normalize=True))

print(f"\nClass distribution in validation set:")
print(y_val.value_counts(normalize=True))

print("\n Data preprocessing completed! Ready for model building.")


# =========================================================================================
# SECTION 5: BASELINE MODEL BUILDING (DEFAULT HYPERPARAMETERS)
# =========================================================================================

"""
OBJECTIVE: Build multiple classification models with default parameters as baselines

WHY START WITH DEFAULT PARAMETERS?
- Establishes a baseline performance to compare against tuned models
- Helps us identify which algorithms naturally work well with our data
- Saves computational time before investing in hyperparameter tuning
- Some models might already perform well without tuning

MODELS WE'LL TEST:
1. Logistic Regression: Linear model, fast, interpretable
2. Naive Bayes: Probabilistic model, works well with high-dimensional data
3. K-Nearest Neighbors (KNN): Instance-based, non-parametric
4. Decision Tree: Non-linear, interpretable, handles interactions
5. Random Forest: Ensemble of trees, robust, handles complex patterns
6. Multi-Layer Perceptron (MLP): Neural network, captures complex non-linear patterns
"""

print("\n" + "="*80)
print("SECTION 5: BASELINE MODEL BUILDING (DEFAULT HYPERPARAMETERS)")
print("="*80)

# Dictionary to store all models and their results
baseline_models = {}
baseline_results = {}

# Helper function to evaluate models
def evaluate_model(model, X_train, X_val, y_train, y_val, model_name):
    """
    Evaluates a model and returns comprehensive performance metrics

    WHY THESE METRICS?
    - F1-Score: Primary metric, balances precision and recall
    - Accuracy: Overall correctness (can be misleading with imbalanced data)
    - Precision: Of predicted positives, how many are actually positive
    - Recall: Of actual positives, how many did we catch
    - ROC-AUC: Measures model's ability to distinguish between classes
    """
    # Train the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_val)
    y_pred_proba = model.predict_proba(X_val)[:, 1] if hasattr(model, 'predict_proba') else None

    # Calculate metrics
    metrics = {
        'Model': model_name,
        'F1-Score': f1_score(y_val, y_pred),
        'Accuracy': accuracy_score(y_val, y_pred),
        'Precision': precision_score(y_val, y_pred),
        'Recall': recall_score(y_val, y_pred),
        'ROC-AUC': roc_auc_score(y_val, y_pred_proba) if y_pred_proba is not None else np.nan
    }

    return model, metrics


print("\n[5.1] Logistic Regression")
print("-" * 80)
print("""
WHAT IS LOGISTIC REGRESSION?
- A linear model for binary classification
- Uses logistic (sigmoid) function to output probabilities
- Works well when decision boundary is roughly linear

STRENGTHS:
- Fast to train and predict
- Interpretable (can see feature importance through coefficients)
- Works well with linearly separable data
- Less prone to overfitting with regularization

LIMITATIONS:
- Assumes linear relationship between features and log-odds of outcome
- May underperform with complex, non-linear patterns
""")

lr_model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)
lr_model, lr_metrics = evaluate_model(lr_model, X_train, X_val, y_train, y_val,
                                      'Logistic Regression')
baseline_models['Logistic Regression'] = lr_model
baseline_results['Logistic Regression'] = lr_metrics

print(f"  F1-Score: {lr_metrics['F1-Score']:.4f}")
print(f"  Accuracy: {lr_metrics['Accuracy']:.4f}")
print(f"  Precision: {lr_metrics['Precision']:.4f}")
print(f"  Recall: {lr_metrics['Recall']:.4f}")
print(f"  ROC-AUC: {lr_metrics['ROC-AUC']:.4f}")


print("\n[5.2] Naive Bayes")
print("-" * 80)
print("""
WHAT IS NAIVE BAYES?
- Probabilistic classifier based on Bayes' theorem
- Assumes features are independent (naive assumption)
- Calculates probability of each class given the features

STRENGTHS:
- Very fast training and prediction
- Works well with high-dimensional data
- Requires less training data
- Handles irrelevant features well

LIMITATIONS:
- Independence assumption rarely holds in real data
- Can be outperformed when features are correlated
- Sensitive to how features are distributed
""")

nb_model = GaussianNB()
nb_model, nb_metrics = evaluate_model(nb_model, X_train, X_val, y_train, y_val,
                                     'Naive Bayes')
baseline_models['Naive Bayes'] = nb_model
baseline_results['Naive Bayes'] = nb_metrics

print(f"  F1-Score: {nb_metrics['F1-Score']:.4f}")
print(f"  Accuracy: {nb_metrics['Accuracy']:.4f}")
print(f"  Precision: {nb_metrics['Precision']:.4f}")
print(f"  Recall: {nb_metrics['Recall']:.4f}")
print(f"  ROC-AUC: {nb_metrics['ROC-AUC']:.4f}")


print("\n[5.3] K-Nearest Neighbors (KNN)")
print("-" * 80)
print("""
WHAT IS K-NEAREST NEIGHBORS?
- Instance-based learning algorithm
- Classifies based on majority vote of K nearest neighbors
- "Nearest" determined by distance metric (usually Euclidean)

STRENGTHS:
- No training phase (lazy learning)
- Naturally handles multi-class problems
- Can capture complex decision boundaries
- Non-parametric (no assumptions about data distribution)

LIMITATIONS:
- Slow prediction time (must search through all training data)
- Sensitive to feature scaling and irrelevant features
- Suffers from curse of dimensionality
- Performance degrades with noisy data
""")

knn_model = KNeighborsClassifier()
knn_model, knn_metrics = evaluate_model(knn_model, X_train, X_val, y_train, y_val,
                                       'K-Nearest Neighbors')
baseline_models['K-Nearest Neighbors'] = knn_model
baseline_results['K-Nearest Neighbors'] = knn_metrics

print(f"  F1-Score: {knn_metrics['F1-Score']:.4f}")
print(f"  Accuracy: {knn_metrics['Accuracy']:.4f}")
print(f"  Precision: {knn_metrics['Precision']:.4f}")
print(f"  Recall: {knn_metrics['Recall']:.4f}")
print(f"  ROC-AUC: {knn_metrics['ROC-AUC']:.4f}")


print("\n[5.4] Decision Tree")
print("-" * 80)
print("""
WHAT IS DECISION TREE?
- Tree-like model that splits data based on feature values
- Each internal node represents a feature test
- Each leaf node represents a class label
- Makes decisions by traversing from root to leaf

STRENGTHS:
- Highly interpretable (can visualize decision rules)
- Handles both numerical and categorical data
- Captures non-linear relationships and feature interactions
- No need for feature scaling
- Can identify important features

LIMITATIONS:
- Prone to overfitting (especially with deep trees)
- Can be unstable (small data changes lead to different trees)
- Biased toward features with more levels
- May not perform well on smooth decision boundaries
""")

dt_model = DecisionTreeClassifier(random_state=RANDOM_STATE)
dt_model, dt_metrics = evaluate_model(dt_model, X_train, X_val, y_train, y_val,
                                     'Decision Tree')
baseline_models['Decision Tree'] = dt_model
baseline_results['Decision Tree'] = dt_metrics

print(f"  F1-Score: {dt_metrics['F1-Score']:.4f}")
print(f"  Accuracy: {dt_metrics['Accuracy']:.4f}")
print(f"  Precision: {dt_metrics['Precision']:.4f}")
print(f"  Recall: {dt_metrics['Recall']:.4f}")
print(f"  ROC-AUC: {dt_metrics['ROC-AUC']:.4f}")


print("\n[5.5] Random Forest")
print("-" * 80)
print("""
WHAT IS RANDOM FOREST?
- Ensemble method that builds multiple decision trees
- Each tree is trained on a random subset of data (bootstrap sampling)
- Each split considers a random subset of features
- Final prediction is majority vote from all trees

STRENGTHS:
- Highly accurate and robust
- Reduces overfitting compared to single decision tree
- Handles large datasets with high dimensionality
- Provides feature importance rankings
- Works well out-of-the-box with default parameters
- Handles missing values and maintains accuracy

LIMITATIONS:
- Less interpretable than single decision tree
- Slower to train and predict than simple models
- Can be memory intensive with many trees
- May overfit on noisy datasets
""")

rf_model = RandomForestClassifier(random_state=RANDOM_STATE)
rf_model, rf_metrics = evaluate_model(rf_model, X_train, X_val, y_train, y_val,
                                     'Random Forest')
baseline_models['Random Forest'] = rf_model
baseline_results['Random Forest'] = rf_metrics

print(f"  F1-Score: {rf_metrics['F1-Score']:.4f}")
print(f"  Accuracy: {rf_metrics['Accuracy']:.4f}")
print(f"  Precision: {rf_metrics['Precision']:.4f}")
print(f"  Recall: {rf_metrics['Recall']:.4f}")
print(f"  ROC-AUC: {rf_metrics['ROC-AUC']:.4f}")


print("\n[5.6] Multi-Layer Perceptron (Neural Network)")
print("-" * 80)
print("""
WHAT IS MULTI-LAYER PERCEPTRON (MLP)?
- Artificial neural network with multiple layers
- Consists of input layer, hidden layers, and output layer
- Each neuron applies weighted sum + activation function
- Learns through backpropagation algorithm

STRENGTHS:
- Can learn complex, non-linear patterns
- Universal function approximator (can model any function)
- Scales well to large datasets
- Can leverage GPU acceleration

LIMITATIONS:
- Requires careful hyperparameter tuning
- Prone to overfitting without regularization
- Computationally expensive to train
- Black box model (hard to interpret)
- Sensitive to feature scaling
- May get stuck in local minima
""")

mlp_model = MLPClassifier(random_state=RANDOM_STATE, max_iter=1000)
mlp_model, mlp_metrics = evaluate_model(mlp_model, X_train, X_val, y_train, y_val,
                                       'MLP Neural Network')
baseline_models['MLP Neural Network'] = mlp_model
baseline_results['MLP Neural Network'] = mlp_metrics

print(f"  F1-Score: {mlp_metrics['F1-Score']:.4f}")
print(f"  Accuracy: {mlp_metrics['Accuracy']:.4f}")
print(f"  Precision: {mlp_metrics['Precision']:.4f}")
print(f"  Recall: {mlp_metrics['Recall']:.4f}")
print(f"  ROC-AUC: {mlp_metrics['ROC-AUC']:.4f}")


# Compile baseline results
print("\n" + "="*80)
print("BASELINE MODEL COMPARISON")
print("="*80)

baseline_df = pd.DataFrame(baseline_results).T
baseline_df = baseline_df.sort_values('F1-Score', ascending=False)
print("\n", baseline_df.to_string())

# Visualize baseline model performance
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

metrics_to_plot = ['F1-Score', 'Accuracy', 'Precision', 'Recall']
colors_map = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c']

for idx, metric in enumerate(metrics_to_plot):
    ax = axes[idx // 2, idx % 2]

    data = baseline_df[metric].sort_values(ascending=True)
    bars = ax.barh(range(len(data)), data.values, color=colors_map[idx],
                   edgecolor='black', linewidth=2)

    # Add value labels
    for i, (bar, value) in enumerate(zip(bars, data.values)):
        ax.text(value + 0.01, i, f'{value:.4f}',
               va='center', fontsize=11, fontweight='bold')

    ax.set_yticks(range(len(data)))
    ax.set_yticklabels(data.index, fontsize=12, fontweight='bold')
    ax.set_xlabel(metric, fontsize=14, fontweight='bold')
    ax.set_title(f'Baseline Model Comparison: {metric}',
                fontsize=15, fontweight='bold', pad=15)
    ax.set_xlim(0, 1.1)
    ax.grid(axis='x', alpha=0.3)
    ax.tick_params(axis='x', labelsize=11)

plt.tight_layout()
plt.savefig('05_baseline_model_comparison.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"\n Baseline comparison plot saved as '05_baseline_model_comparison.png'")

"""
INTERPRETATION OF BASELINE RESULTS:
-----------------------------------
- The model with highest F1-score is our best baseline performer
- If F1-score and accuracy differ significantly, we have class imbalance issues
- High precision but low recall: Model is conservative (misses positive cases)
- High recall but low precision: Model is aggressive (many false positives)
- ROC-AUC close to 0.5: Model performs no better than random guessing
- ROC-AUC close to 1.0: Model has excellent discriminative ability
"""


# =========================================================================================
# SECTION 6: HYPERPARAMETER TUNING WITH CROSS-VALIDATION
# =========================================================================================

"""
OBJECTIVE: Optimize each model's hyperparameters to improve performance

WHY HYPERPARAMETER TUNING?
- Default parameters are rarely optimal for specific datasets
- Tuning can significantly improve model performance
- Helps balance bias-variance tradeoff
- Prevents overfitting through proper regularization

APPROACH: GridSearchCV with 3-Fold Cross-Validation
- Grid Search: Tests all combinations of specified parameters
- Cross-Validation (K=3): Splits training data into 3 folds
  * Each fold serves as validation set once
  * Averages performance across folds for robust estimates
  * Reduces risk of overfitting to a single train-val split

WHY K=3?
- Balance between computational cost and reliable estimates
- With limited data, preserves more training samples per fold
- Higher K (e.g., 5, 10) is better with larger datasets
"""

print("\n" + "="*80)
print("SECTION 6: HYPERPARAMETER TUNING WITH CROSS-VALIDATION (K=3)")
print("="*80)

# Dictionary to store tuned models and results
tuned_models = {}
tuned_results = {}

# Combine train and validation for cross-validation
# (Cross-validation will split internally)
X_train_full = pd.concat([X_train, X_val])
y_train_full = pd.concat([y_train, y_val])

print(f"\nUsing full training data for cross-validation:")
print(f"  Total samples: {len(X_train_full)}")
print(f"  Cross-validation folds: 3")
print(f"  Scoring metric: F1-Score")


print("\n[6.1] Tuning Logistic Regression")
print("-" * 80)
print("""
HYPERPARAMETERS TO TUNE:
1. C (Regularization strength):
   - Inverse of regularization strength (smaller = stronger regularization)
   - Controls overfitting (small C = more regularization = less overfitting)
   - Range: Try values from 0.01 to 100

2. penalty (Regularization type):
   - 'l2': Ridge regularization (shrinks coefficients)
   - 'l1': Lasso regularization (can zero out coefficients)

3. solver (Optimization algorithm):
   - 'liblinear': Good for small datasets, supports l1 penalty
   - 'saga': Supports l1 and l2, good for large datasets
""")

lr_param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],
    'penalty': ['l2'],  # l1 requires specific solvers
    'solver': ['liblinear', 'saga']
}

lr_grid = GridSearchCV(
    LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),
    lr_param_grid,
    cv=3,
    scoring='f1',
    n_jobs=-1,
    verbose=1
)

print("\nStarting grid search...")
lr_grid.fit(X_train_full, y_train_full)

print(f"\n Best parameters: {lr_grid.best_params_}")
print(f" Best cross-validation F1-score: {lr_grid.best_score_:.4f}")

# Evaluate on validation set
lr_tuned_pred = lr_grid.predict(X_val)
lr_tuned_proba = lr_grid.predict_proba(X_val)[:, 1]

tuned_results['Logistic Regression (Tuned)'] = {
    'Model': 'Logistic Regression (Tuned)',
    'F1-Score': f1_score(y_val, lr_tuned_pred),
    'Accuracy': accuracy_score(y_val, lr_tuned_pred),
    'Precision': precision_score(y_val, lr_tuned_pred),
    'Recall': recall_score(y_val, lr_tuned_pred),
    'ROC-AUC': roc_auc_score(y_val, lr_tuned_proba)
}
tuned_models['Logistic Regression (Tuned)'] = lr_grid.best_estimator_

print(f"\nValidation Performance:")
for metric, value in tuned_results['Logistic Regression (Tuned)'].items():
    if metric != 'Model':
        print(f"  {metric}: {value:.4f}")


print("\n[6.2] Tuning Naive Bayes")
print("-" * 80)
print("""
HYPERPARAMETERS TO TUNE:
1. var_smoothing:
   - Portion of largest variance added to variances for stability
   - Prevents zero probabilities
   - Range: Very small values (1e-9 to 1e-7)

NOTE: Naive Bayes has limited hyperparameters compared to other models
The algorithm is relatively simple and often works well with defaults
""")

nb_param_grid = {
    'var_smoothing': np.logspace(-9, -7, 5)
}

nb_grid = GridSearchCV(
    GaussianNB(),
    nb_param_grid,
    cv=3,
    scoring='f1',
    n_jobs=-1,
    verbose=1
)

print("\nStarting grid search...")
nb_grid.fit(X_train_full, y_train_full)

print(f"\n Best parameters: {nb_grid.best_params_}")
print(f" Best cross-validation F1-score: {nb_grid.best_score_:.4f}")

nb_tuned_pred = nb_grid.predict(X_val)
nb_tuned_proba = nb_grid.predict_proba(X_val)[:, 1]

tuned_results['Naive Bayes (Tuned)'] = {
    'Model': 'Naive Bayes (Tuned)',
    'F1-Score': f1_score(y_val, nb_tuned_pred),
    'Accuracy': accuracy_score(y_val, nb_tuned_pred),
    'Precision': precision_score(y_val, nb_tuned_pred),
    'Recall': recall_score(y_val, nb_tuned_pred),
    'ROC-AUC': roc_auc_score(y_val, nb_tuned_proba)
}
tuned_models['Naive Bayes (Tuned)'] = nb_grid.best_estimator_

print(f"\nValidation Performance:")
for metric, value in tuned_results['Naive Bayes (Tuned)'].items():
    if metric != 'Model':
        print(f"  {metric}: {value:.4f}")


print("\n[6.3] Tuning K-Nearest Neighbors")
print("-" * 80)
print("""
HYPERPARAMETERS TO TUNE:
1. n_neighbors (K):
   - Number of neighbors to consider
   - Small K: More complex boundary, prone to noise
   - Large K: Smoother boundary, may underfit
   - Typically odd numbers to avoid ties

2. weights:
   - 'uniform': All neighbors equally weighted
   - 'distance': Closer neighbors have more influence

3. metric:
   - 'euclidean': Straight-line distance
   - 'manhattan': Sum of absolute differences
   - 'minkowski': Generalization of above
""")

knn_param_grid = {
    'n_neighbors': [3, 5, 7, 9, 11, 15],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}

knn_grid = GridSearchCV(
    KNeighborsClassifier(),
    knn_param_grid,
    cv=3,
    scoring='f1',
    n_jobs=-1,
    verbose=1
)

print("\nStarting grid search...")
knn_grid.fit(X_train_full, y_train_full)

print(f"\n Best parameters: {knn_grid.best_params_}")
print(f" Best cross-validation F1-score: {knn_grid.best_score_:.4f}")

knn_tuned_pred = knn_grid.predict(X_val)
knn_tuned_proba = knn_grid.predict_proba(X_val)[:, 1]

tuned_results['K-Nearest Neighbors (Tuned)'] = {
    'Model': 'K-Nearest Neighbors (Tuned)',
    'F1-Score': f1_score(y_val, knn_tuned_pred),
    'Accuracy': accuracy_score(y_val, knn_tuned_pred),
    'Precision': precision_score(y_val, knn_tuned_pred),
    'Recall': recall_score(y_val, knn_tuned_pred),
    'ROC-AUC': roc_auc_score(y_val, knn_tuned_proba)
}
tuned_models['K-Nearest Neighbors (Tuned)'] = knn_grid.best_estimator_

print(f"\nValidation Performance:")
for metric, value in tuned_results['K-Nearest Neighbors (Tuned)'].items():
    if metric != 'Model':
        print(f"  {metric}: {value:.4f}")


print("\n[6.4] Tuning Decision Tree")
print("-" * 80)
print("""
HYPERPARAMETERS TO TUNE:
1. max_depth:
   - Maximum depth of the tree
   - Deeper trees can model complex patterns but may overfit
   - Shallower trees are more generalizable but may underfit

2. min_samples_split:
   - Minimum samples required to split a node
   - Higher values prevent overfitting by creating less specific rules

3. min_samples_leaf:
   - Minimum samples required in a leaf node
   - Smooths the model and prevents over-specific predictions

4. criterion:
   - 'gini': Gini impurity (default, slightly faster)
   - 'entropy': Information gain (may give slightly different results)
""")

dt_param_grid = {
    'max_depth': [3, 5, 7, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy']
}

dt_grid = GridSearchCV(
    DecisionTreeClassifier(random_state=RANDOM_STATE),
    dt_param_grid,
    cv=3,
    scoring='f1',
    n_jobs=-1,
    verbose=1
)

print("\nStarting grid search...")
dt_grid.fit(X_train_full, y_train_full)

print(f"\n Best parameters: {dt_grid.best_params_}")
print(f" Best cross-validation F1-score: {dt_grid.best_score_:.4f}")

dt_tuned_pred = dt_grid.predict(X_val)
dt_tuned_proba = dt_grid.predict_proba(X_val)[:, 1]

tuned_results['Decision Tree (Tuned)'] = {
    'Model': 'Decision Tree (Tuned)',
    'F1-Score': f1_score(y_val, dt_tuned_pred),
    'Accuracy': accuracy_score(y_val, dt_tuned_pred),
    'Precision': precision_score(y_val, dt_tuned_pred),
    'Recall': recall_score(y_val, dt_tuned_pred),
    'ROC-AUC': roc_auc_score(y_val, dt_tuned_proba)
}
tuned_models['Decision Tree (Tuned)'] = dt_grid.best_estimator_

print(f"\nValidation Performance:")
for metric, value in tuned_results['Decision Tree (Tuned)'].items():
    if metric != 'Model':
        print(f"  {metric}: {value:.4f}")


print("\n[6.5] Tuning Random Forest")
print("-" * 80)
print("""
HYPERPARAMETERS TO TUNE:
1. n_estimators:
   - Number of trees in the forest
   - More trees generally improve performance but increase computation
   - Diminishing returns after certain point

2. max_depth:
   - Maximum depth of each tree
   - Controls complexity of individual trees

3. min_samples_split:
   - Minimum samples to split a node
   - Higher values create simpler trees

4. min_samples_leaf:
   - Minimum samples in leaf nodes
   - Controls tree complexity

5. max_features:
   - Number of features to consider for best split
   - 'sqrt': Square root of total features (default, good balance)
   - 'log2': Log base 2 of total features (more random)
   - Lower values increase randomness and reduce correlation between trees
""")

rf_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['sqrt', 'log2']
}

rf_grid = GridSearchCV(
    RandomForestClassifier(random_state=RANDOM_STATE),
    rf_param_grid,
    cv=3,
    scoring='f1',
    n_jobs=-1,
    verbose=1
)

print("\nStarting grid search...")
print("(This may take a few minutes due to ensemble complexity...)")
rf_grid.fit(X_train_full, y_train_full)

print(f"\n Best parameters: {rf_grid.best_params_}")
print(f" Best cross-validation F1-score: {rf_grid.best_score_:.4f}")

rf_tuned_pred = rf_grid.predict(X_val)
rf_tuned_proba = rf_grid.predict_proba(X_val)[:, 1]

tuned_results['Random Forest (Tuned)'] = {
    'Model': 'Random Forest (Tuned)',
    'F1-Score': f1_score(y_val, rf_tuned_pred),
    'Accuracy': accuracy_score(y_val, rf_tuned_pred),
    'Precision': precision_score(y_val, rf_tuned_pred),
    'Recall': recall_score(y_val, rf_tuned_pred),
    'ROC-AUC': roc_auc_score(y_val, rf_tuned_proba)
}
tuned_models['Random Forest (Tuned)'] = rf_grid.best_estimator_

print(f"\nValidation Performance:")
for metric, value in tuned_results['Random Forest (Tuned)'].items():
    if metric != 'Model':
        print(f"  {metric}: {value:.4f}")


print("\n[6.6] Tuning Multi-Layer Perceptron")
print("-" * 80)
print("""
HYPERPARAMETERS TO TUNE:
1. hidden_layer_sizes:
   - Architecture of hidden layers (e.g., (100,) = 1 layer with 100 neurons)
   - More neurons/layers can learn complex patterns but risk overfitting

2. activation:
   - 'relu': Rectified Linear Unit (most common, good default)
   - 'tanh': Hyperbolic tangent (bounded output)

3. alpha:
   - L2 regularization parameter
   - Higher values prevent overfitting

4. learning_rate:
   - 'constant': Fixed learning rate
   - 'adaptive': Decreases learning rate when training stagnates

5. max_iter:
   - Maximum training iterations
   - More iterations allow better convergence
""")

mlp_param_grid = {
    'hidden_layer_sizes': [(50,), (100,), (100, 50), (100, 100)],
    'activation': ['relu', 'tanh'],
    'alpha': [0.0001, 0.001, 0.01],
    'learning_rate': ['constant', 'adaptive']
}

mlp_grid = GridSearchCV(
    MLPClassifier(random_state=RANDOM_STATE, max_iter=1000),
    mlp_param_grid,
    cv=3,
    scoring='f1',
    n_jobs=-1,
    verbose=1
)

print("\nStarting grid search...")
print("(Neural network tuning may take several minutes...)")
mlp_grid.fit(X_train_full, y_train_full)

print(f"\n Best parameters: {mlp_grid.best_params_}")
print(f" Best cross-validation F1-score: {mlp_grid.best_score_:.4f}")

mlp_tuned_pred = mlp_grid.predict(X_val)
mlp_tuned_proba = mlp_grid.predict_proba(X_val)[:, 1]

tuned_results['MLP Neural Network (Tuned)'] = {
    'Model': 'MLP Neural Network (Tuned)',
    'F1-Score': f1_score(y_val, mlp_tuned_pred),
    'Accuracy': accuracy_score(y_val, mlp_tuned_pred),
    'Precision': precision_score(y_val, mlp_tuned_pred),
    'Recall': recall_score(y_val, mlp_tuned_pred),
    'ROC-AUC': roc_auc_score(y_val, mlp_tuned_proba)
}
tuned_models['MLP Neural Network (Tuned)'] = mlp_grid.best_estimator_

print(f"\nValidation Performance:")
for metric, value in tuned_results['MLP Neural Network (Tuned)'].items():
    if metric != 'Model':
        print(f"  {metric}: {value:.4f}")


# Compile tuned model results
print("\n" + "="*80)
print("TUNED MODEL COMPARISON")
print("="*80)

tuned_df = pd.DataFrame(tuned_results).T
tuned_df = tuned_df.sort_values('F1-Score', ascending=False)
print("\n", tuned_df.to_string())


# =========================================================================================
# SECTION 7: BASELINE VS TUNED COMPARISON
# =========================================================================================

"""
OBJECTIVE: Compare baseline and tuned models to assess tuning impact

WHY THIS COMPARISON MATTERS:
- Shows which models benefited most from tuning
- Helps us understand if the default parameters were already near-optimal
- Validates that our tuning effort was worthwhile
- Some models may not improve much (already well-suited to the data)
"""

print("\n" + "="*80)
print("SECTION 7: BASELINE VS TUNED MODEL COMPARISON")
print("="*80)

# Combine baseline and tuned results
all_results = {**baseline_results, **tuned_results}
comparison_df = pd.DataFrame(all_results).T
comparison_df = comparison_df.sort_values('F1-Score', ascending=False)

print("\nComplete Model Comparison (Sorted by F1-Score):")
print(comparison_df.to_string())

# Calculate improvement from tuning
print("\n" + "-"*80)
print("IMPROVEMENT FROM HYPERPARAMETER TUNING")
print("-"*80)

for model_name in baseline_results.keys():
    baseline_f1 = baseline_results[model_name]['F1-Score']
    tuned_name = f"{model_name} (Tuned)"

    if tuned_name in tuned_results:
        tuned_f1 = tuned_results[tuned_name]['F1-Score']
        improvement = ((tuned_f1 - baseline_f1) / baseline_f1) * 100

        print(f"\n{model_name}:")
        print(f"  Baseline F1: {baseline_f1:.4f}")
        print(f"  Tuned F1: {tuned_f1:.4f}")
        print(f"  Improvement: {improvement:+.2f}%")

# Visualize baseline vs tuned comparison
fig, ax = plt.subplots(figsize=(14, 8))

model_names = list(baseline_results.keys())
baseline_f1_scores = [baseline_results[m]['F1-Score'] for m in model_names]
tuned_f1_scores = [tuned_results[f"{m} (Tuned)"]['F1-Score'] for m in model_names]

x = np.arange(len(model_names))
width = 0.35

bars1 = ax.bar(x - width/2, baseline_f1_scores, width, label='Baseline (Default)',
               color='#3498db', edgecolor='black', linewidth=2)
bars2 = ax.bar(x + width/2, tuned_f1_scores, width, label='Tuned (Optimized)',
               color='#2ecc71', edgecolor='black', linewidth=2)

# Add value labels
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.4f}',
                ha='center', va='bottom', fontsize=10, fontweight='bold')

ax.set_xlabel('Model', fontsize=15, fontweight='bold')
ax.set_ylabel('F1-Score', fontsize=15, fontweight='bold')
ax.set_title('Baseline vs Tuned Model Performance Comparison',
             fontsize=16, fontweight='bold', pad=20)
ax.set_xticks(x)
ax.set_xticklabels(model_names, fontsize=12, fontweight='bold', rotation=45, ha='right')
ax.set_yticks(np.arange(0, 1.1, 0.1))
ax.tick_params(axis='y', labelsize=13)
ax.legend(fontsize=13, loc='best')
ax.grid(axis='y', alpha=0.3)
ax.set_ylim(0, max(max(baseline_f1_scores), max(tuned_f1_scores)) + 0.1)

plt.tight_layout()
plt.savefig('06_baseline_vs_tuned_comparison.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"\n Comparison plot saved as '06_baseline_vs_tuned_comparison.png'")


# =========================================================================================
# SECTION 8: SELECTING TOP 4 MODELS FOR ENSEMBLE
# =========================================================================================

"""
OBJECTIVE: Select the best 4 models to combine in ensemble methods

WHY SELECT TOP 4 MODELS?
- Ensemble methods combine multiple models for better predictions
- Using too many models can add noise and computational cost
- Top performers are most likely to contribute meaningfully
- Diversity in model types helps ensemble performance

SELECTION CRITERIA:
- Primary: F1-Score (our main evaluation metric)
- Consider: Balance between different model types for diversity
- Prefer: Tuned versions (they've been optimized for our data)
"""

print("\n" + "="*80)
print("SECTION 8: SELECTING TOP 4 MODELS FOR ENSEMBLE")
print("="*80)

# Sort all models by F1-score and select top 4
top_4_models = comparison_df.nlargest(4, 'F1-Score')

print("\nTop 4 Models Selected (Based on F1-Score):")
print(top_4_models[['F1-Score', 'Accuracy', 'Precision', 'Recall', 'ROC-AUC']].to_string())

print("\n" + "-"*80)
print("WHY THESE MODELS WERE SELECTED:")
print("-"*80)

for idx, (model_name, row) in enumerate(top_4_models.iterrows(), 1):
    print(f"\n{idx}. {model_name}")
    print(f"   F1-Score: {row['F1-Score']:.4f}")
    print(f"   Strengths: High F1-score indicates good balance of precision and recall")
    print(f"   Role in Ensemble: Contributes its prediction to final voting mechanism")

# Store top 4 model objects for ensemble
top_4_model_objects = []
top_4_model_names = []

for model_name in top_4_models.index:
    # Check if it's a tuned model
    if '(Tuned)' in model_name:
        top_4_model_objects.append(tuned_models[model_name])
    else:
        top_4_model_objects.append(baseline_models[model_name])
    top_4_model_names.append(model_name)

print(f"\n Top 4 models ready for ensemble building!")


# =========================================================================================
# SECTION 9: BUILDING ENSEMBLE MODELS
# =========================================================================================

"""
OBJECTIVE: Combine top 4 models using ensemble techniques

WHY ENSEMBLE METHODS?
- Individual models have different strengths and weaknesses
- Combining predictions can reduce errors and improve robustness
- "Wisdom of crowds" principle: collective prediction often beats individual

TWO ENSEMBLE APPROACHES:

1. EQUAL VOTING (Hard Voting):
   - Each model gets one vote
   - Final prediction is majority vote
   - Simple, democratic approach
   - Works well when models have similar performance

2. WEIGHTED VOTING (Soft Voting):
   - Models vote based on their predicted probabilities
   - Better models can have more influence
   - More sophisticated than hard voting
   - Can capture uncertainty better
"""

print("\n" + "="*80)
print("SECTION 9: BUILDING ENSEMBLE MODELS")
print("="*80)

ensemble_results = {}

# 9.1: Equal Voting Ensemble (Hard Voting)
print("\n[9.1] Equal Voting Ensemble (Hard Voting)")
print("-" * 80)
print("""
HOW EQUAL VOTING WORKS:
- Each model makes a prediction (0 or 1)
- Count votes for each class
- Final prediction is the class with most votes
- Example: If 3 models predict 1 and 1 model predicts 0, final prediction is 1

ADVANTAGES:
- Simple and interpretable
- Reduces variance (less likely to make extreme predictions)
- No assumptions about model quality differences

DISADVANTAGES:
- Treats all models equally (even if some are much better)
- Doesn't consider model confidence
- Can be suboptimal if models have very different performance levels
""")

# Create equal voting ensemble
equal_voting_ensemble = VotingClassifier(
    estimators=[(name, model) for name, model in zip(top_4_model_names, top_4_model_objects)],
    voting='hard'  # Hard voting = majority vote
)

# Train ensemble
print("\nTraining equal voting ensemble...")
equal_voting_ensemble.fit(X_train_full, y_train_full)

# Make predictions
equal_voting_pred = equal_voting_ensemble.predict(X_val)

# Note: Hard voting doesn't provide probability estimates by default
ensemble_results['Equal Voting Ensemble'] = {
    'Model': 'Equal Voting Ensemble',
    'F1-Score': f1_score(y_val, equal_voting_pred),
    'Accuracy': accuracy_score(y_val, equal_voting_pred),
    'Precision': precision_score(y_val, equal_voting_pred),
    'Recall': recall_score(y_val, equal_voting_pred),
    'ROC-AUC': np.nan  # Hard voting doesn't produce probabilities
}

print(f"\n Equal Voting Ensemble Performance:")
for metric, value in ensemble_results['Equal Voting Ensemble'].items():
    if metric != 'Model' and not np.isnan(value):
        print(f"  {metric}: {value:.4f}")


# 9.2: Weighted Voting Ensemble (Soft Voting)
print("\n[9.2] Weighted Voting Ensemble (Soft Voting)")
print("-" * 80)
print("""
HOW WEIGHTED VOTING WORKS:
- Each model outputs probability estimates (e.g., 0.7 for class 1, 0.3 for class 0)
- Average the probabilities across all models
- Final prediction is class with highest average probability
- Example:
  * Model 1: [0.3, 0.7] (70% confident in class 1)
  * Model 2: [0.4, 0.6] (60% confident in class 1)
  * Model 3: [0.2, 0.8] (80% confident in class 1)
  * Model 4: [0.5, 0.5] (uncertain)
  * Average: [0.35, 0.65] → Predict class 1

ADVANTAGES:
- Leverages probability estimates (richer information)
- Can capture model confidence
- Generally performs better than hard voting
- Provides probability estimates for downstream use

DISADVANTAGES:
- Requires all models to support probability estimation
- More computationally expensive
- Can be dominated by overconfident models
""")

# Create weighted voting ensemble
weighted_voting_ensemble = VotingClassifier(
    estimators=[(name, model) for name, model in zip(top_4_model_names, top_4_model_objects)],
    voting='soft'  # Soft voting = probability averaging
)

# Train ensemble
print("\nTraining weighted voting ensemble...")
weighted_voting_ensemble.fit(X_train_full, y_train_full)

# Make predictions
weighted_voting_pred = weighted_voting_ensemble.predict(X_val)
weighted_voting_proba = weighted_voting_ensemble.predict_proba(X_val)[:, 1]

ensemble_results['Weighted Voting Ensemble'] = {
    'Model': 'Weighted Voting Ensemble',
    'F1-Score': f1_score(y_val, weighted_voting_pred),
    'Accuracy': accuracy_score(y_val, weighted_voting_pred),
    'Precision': precision_score(y_val, weighted_voting_pred),
    'Recall': recall_score(y_val, weighted_voting_pred),
    'ROC-AUC': roc_auc_score(y_val, weighted_voting_proba)
}

print(f"\n Weighted Voting Ensemble Performance:")
for metric, value in ensemble_results['Weighted Voting Ensemble'].items():
    if metric != 'Model':
        print(f"  {metric}: {value:.4f}")


print("\n" + "="*80)
print("ENSEMBLE MODEL COMPARISON")
print("="*80)

ensemble_df = pd.DataFrame(ensemble_results).T
print("\n", ensemble_df.to_string())

"""
EXPECTED OUTCOMES:
- Weighted voting typically performs better than equal voting
- Ensemble models often (but not always) outperform individual models
- If ensemble doesn't improve, top individual model might be sufficient
- Improvement depends on model diversity and individual model quality
"""


# =========================================================================================
# SECTION 10: FINAL MODEL COMPARISON AND SELECTION
# =========================================================================================

"""
OBJECTIVE: Compare all models and select the best performer

WHY COMPREHENSIVE COMPARISON?
- Ensures we choose the truly best model for deployment
- Allows us to see the full landscape of model performance
- Helps us understand trade-offs between different approaches
- Provides transparency in model selection process

DECISION CRITERIA:
- Primary: F1-Score (balances precision and recall)
- Secondary: Consider interpretability, deployment complexity, inference speed
- Final choice depends on business requirements and constraints
"""

print("\n" + "="*80)
print("SECTION 10: FINAL MODEL COMPARISON AND SELECTION")
print("="*80)

# Combine all results
final_results = {**all_results, **ensemble_results}
final_comparison_df = pd.DataFrame(final_results).T
final_comparison_df = final_comparison_df.sort_values('F1-Score', ascending=False)

print("\nComplete Model Comparison - All Models (Sorted by F1-Score):")
print(final_comparison_df.to_string())

# Identify best model
best_model_name = final_comparison_df.index[0]
best_model_metrics = final_comparison_df.iloc[0]

print("\n" + "="*80)
print("BEST MODEL SELECTION")
print("="*80)

print(f"\n SELECTED MODEL: {best_model_name}")
print(f"\n{'='*80}")
print("PERFORMANCE METRICS:")
print(f"{'='*80}")
for metric, value in best_model_metrics.items():
    if metric != 'Model' and not (isinstance(value, float) and np.isnan(value)):
        print(f"  {metric}: {value:.4f}")

print(f"\n{'='*80}")
print("WHY THIS MODEL WAS SELECTED:")
print(f"{'='*80}")
print(f"""
1. HIGHEST F1-SCORE: {best_model_metrics['F1-Score']:.4f}
   - Best balance between precision and recall
   - Minimizes both false positives and false negatives

2. BALANCED PERFORMANCE:
   - Precision: {best_model_metrics['Precision']:.4f} (Low false positive rate)
   - Recall: {best_model_metrics['Recall']:.4f} (Catches most positive cases)

3. STRONG DISCRIMINATIVE ABILITY:
   - ROC-AUC: {best_model_metrics.get('ROC-AUC', 'N/A')}
   - Can effectively separate paid vs unpaid loans

4. BUSINESS IMPACT:
   - In loan prediction, F1-score is critical because:
     * False Positives (approve bad loan): Direct financial loss
     * False Negatives (reject good borrower): Lost business opportunity
   - This model provides the best balance for business decisions
""")

# Visualize final comparison
fig, axes = plt.subplots(2, 2, figsize=(18, 14))

# Prepare data for visualization
models_subset = final_comparison_df.head(10)  # Top 10 for clarity

# Plot 1: F1-Score comparison
ax = axes[0, 0]
colors = ['#2ecc71' if i == 0 else '#3498db' for i in range(len(models_subset))]
bars = ax.barh(range(len(models_subset)), models_subset['F1-Score'],
               color=colors, edgecolor='black', linewidth=2)
for i, (bar, value) in enumerate(zip(bars, models_subset['F1-Score'])):
    ax.text(value + 0.005, i, f'{value:.4f}', va='center', fontsize=10, fontweight='bold')
ax.set_yticks(range(len(models_subset)))
ax.set_yticklabels(models_subset.index, fontsize=11, fontweight='bold')
ax.set_xlabel('F1-Score', fontsize=14, fontweight='bold')
ax.set_title('Top 10 Models: F1-Score Comparison', fontsize=15, fontweight='bold', pad=15)
ax.grid(axis='x', alpha=0.3)
ax.set_xlim(0, 1.1)

# Plot 2: Precision vs Recall scatter
ax = axes[0, 1]
scatter = ax.scatter(models_subset['Precision'], models_subset['Recall'],
                    s=200, alpha=0.6, c=range(len(models_subset)),
                    cmap='viridis', edgecolors='black', linewidth=2)
for idx, model_name in enumerate(models_subset.index):
    ax.annotate(str(idx+1),
                (models_subset.loc[model_name, 'Precision'],
                 models_subset.loc[model_name, 'Recall']),
                fontsize=12, fontweight='bold', ha='center', va='center')
ax.set_xlabel('Precision', fontsize=14, fontweight='bold')
ax.set_ylabel('Recall', fontsize=14, fontweight='bold')
ax.set_title('Precision-Recall Trade-off', fontsize=15, fontweight='bold', pad=15)
ax.grid(alpha=0.3)
ax.set_xlim(0, 1.05)
ax.set_ylim(0, 1.05)

# Plot 3: Multiple metrics comparison for top 5
ax = axes[1, 0]
top_5 = models_subset.head(5)
metrics = ['F1-Score', 'Accuracy', 'Precision', 'Recall']
x = np.arange(len(top_5))
width = 0.2

for i, metric in enumerate(metrics):
    offset = (i - 1.5) * width
    ax.bar(x + offset, top_5[metric], width, label=metric, edgecolor='black', linewidth=1.5)

ax.set_xlabel('Model Rank', fontsize=14, fontweight='bold')
ax.set_ylabel('Score', fontsize=14, fontweight='bold')
ax.set_title('Top 5 Models: Multi-Metric Comparison', fontsize=15, fontweight='bold', pad=15)
ax.set_xticks(x)
ax.set_xticklabels([f'#{i+1}' for i in range(len(top_5))], fontsize=12, fontweight='bold')
ax.legend(fontsize=12, loc='best')
ax.grid(axis='y', alpha=0.3)
ax.set_ylim(0, 1.1)

# Plot 4: Model type performance summary
ax = axes[1, 1]
model_types = {
    'Logistic Regression': [],
    'Naive Bayes': [],
    'K-Nearest Neighbors': [],
    'Decision Tree': [],
    'Random Forest': [],
    'MLP Neural Network': [],
    'Ensemble': []
}

for model_name, row in final_comparison_df.iterrows():
    for model_type in model_types.keys():
        if model_type in model_name or (model_type == 'Ensemble' and 'Ensemble' in model_name):
            model_types[model_type].append(row['F1-Score'])

avg_scores = {k: np.mean(v) if v else 0 for k, v in model_types.items()}
avg_scores = dict(sorted(avg_scores.items(), key=lambda x: x[1], reverse=True))

bars = ax.barh(range(len(avg_scores)), list(avg_scores.values()),
               color='#e74c3c', edgecolor='black', linewidth=2)
for i, (bar, value) in enumerate(zip(bars, avg_scores.values())):
    if value > 0:
        ax.text(value + 0.01, i, f'{value:.4f}', va='center', fontsize=11, fontweight='bold')

ax.set_yticks(range(len(avg_scores)))
ax.set_yticklabels(list(avg_scores.keys()), fontsize=12, fontweight='bold')
ax.set_xlabel('Average F1-Score', fontsize=14, fontweight='bold')
ax.set_title('Algorithm Type Performance Summary', fontsize=15, fontweight='bold', pad=15)
ax.grid(axis='x', alpha=0.3)
ax.set_xlim(0, 1.1)

plt.tight_layout()
plt.savefig('07_final_model_comparison.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"\n Final comparison plot saved as '07_final_model_comparison.png'")


# =========================================================================================
# SECTION 11: FINAL MODEL ANALYSIS
# =========================================================================================

"""
OBJECTIVE: Perform detailed analysis of the selected best model

WHY ANALYZE THE FINAL MODEL?
- Understand how the model makes decisions
- Identify which features are most important
- Visualize model performance through various metrics
- Build trust and interpretability for stakeholders
- Ensure model is ready for deployment
"""

print("\n" + "="*80)
print("SECTION 11: DETAILED ANALYSIS OF BEST MODEL")
print("="*80)

# Get the best model object
if 'Ensemble' in best_model_name:
    if 'Equal' in best_model_name:
        best_model = equal_voting_ensemble
    else:
        best_model = weighted_voting_ensemble
elif '(Tuned)' in best_model_name:
    best_model = tuned_models[best_model_name]
else:
    best_model = baseline_models[best_model_name]


# 11.1: Confusion Matrix
print("\n[11.1] Confusion Matrix Analysis")
print("-" * 80)
print("""
WHAT IS A CONFUSION MATRIX?
- Shows actual vs predicted classifications
- Four outcomes:
  * True Positives (TP): Correctly predicted paid loans
  * True Negatives (TN): Correctly predicted unpaid loans
  * False Positives (FP): Predicted paid but actually unpaid (Type I error)
  * False Negatives (FN): Predicted unpaid but actually paid (Type II error)

BUSINESS INTERPRETATION:
- FP (False Positive): Approved a loan that will default → Financial loss
- FN (False Negative): Rejected a good borrower → Lost revenue opportunity
- The ideal model minimizes both types of errors
""")

# Generate predictions for confusion matrix
y_pred_best = best_model.predict(X_val)
cm = confusion_matrix(y_val, y_pred_best)

# Visualize confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,
            square=True, linewidths=2, linecolor='black',
            annot_kws={'fontsize': 16, 'fontweight': 'bold'})
plt.xlabel('Predicted Label', fontsize=15, fontweight='bold')
plt.ylabel('Actual Label', fontsize=15, fontweight='bold')
plt.title(f'Confusion Matrix: {best_model_name}', fontsize=16, fontweight='bold', pad=20)
plt.xticks([0.5, 1.5], ['Not Paid (0)', 'Paid (1)'], fontsize=13, fontweight='bold')
plt.yticks([0.5, 1.5], ['Not Paid (0)', 'Paid (1)'], fontsize=13, fontweight='bold', rotation=0)
plt.tight_layout()
plt.savefig('08_confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"\n Confusion matrix saved as '08_confusion_matrix.png'")

# Print confusion matrix interpretation
tn, fp, fn, tp = cm.ravel()
print(f"\nConfusion Matrix Breakdown:")
print(f"  True Negatives (TN):  {tn:4d} - Correctly predicted unpaid loans")
print(f"  False Positives (FP): {fp:4d} - Incorrectly approved loans (will default)")
print(f"  False Negatives (FN): {fn:4d} - Incorrectly rejected loans (lost customers)")
print(f"  True Positives (TP):  {tp:4d} - Correctly predicted paid loans")

print(f"\nError Analysis:")
print(f"  Type I Error Rate (FP Rate):  {fp/(fp+tn)*100:.2f}%")
print(f"  Type II Error Rate (FN Rate): {fn/(fn+tp)*100:.2f}%")


# 11.2: ROC Curve
print("\n[11.2] ROC Curve Analysis")
print("-" * 80)
print("""
WHAT IS AN ROC CURVE?
- ROC = Receiver Operating Characteristic
- Plots True Positive Rate (TPR) vs False Positive Rate (FPR)
- Shows model performance across all classification thresholds
- Area Under Curve (AUC) measures overall discriminative ability

INTERPRETING ROC-AUC:
- AUC = 0.5: No better than random guessing (diagonal line)
- AUC = 0.7-0.8: Acceptable performance
- AUC = 0.8-0.9: Excellent performance
- AUC = 0.9-1.0: Outstanding performance
- AUC = 1.0: Perfect classifier

WHY IT MATTERS:
- Threshold-independent metric (not affected by classification cutoff)
- Good for comparing different models
- Useful when class distribution might change in production
""")

if hasattr(best_model, 'predict_proba'):
    y_pred_proba_best = best_model.predict_proba(X_val)[:, 1]
    fpr, tpr, thresholds = roc_curve(y_val, y_pred_proba_best)
    roc_auc = roc_auc_score(y_val, y_pred_proba_best)

    plt.figure(figsize=(10, 8))
    plt.plot(fpr, tpr, color='#2ecc71', linewidth=3, label=f'ROC Curve (AUC = {roc_auc:.4f})')
    plt.plot([0, 1], [0, 1], color='#e74c3c', linestyle='--', linewidth=2, label='Random Classifier (AUC = 0.5)')
    plt.fill_between(fpr, tpr, alpha=0.3, color='#2ecc71')

    plt.xlabel('False Positive Rate', fontsize=15, fontweight='bold')
    plt.ylabel('True Positive Rate', fontsize=15, fontweight='bold')
    plt.title(f'ROC Curve: {best_model_name}', fontsize=16, fontweight='bold', pad=20)
    plt.legend(fontsize=13, loc='lower right')
    plt.grid(alpha=0.3)
    plt.xticks(fontsize=13, fontweight='bold')
    plt.yticks(fontsize=13, fontweight='bold')
    plt.tight_layout()
    plt.savefig('09_roc_curve.png', dpi=300, bbox_inches='tight')
    plt.show()

    print(f"\n ROC curve saved as '09_roc_curve.png'")
    print(f"\nROC-AUC Score: {roc_auc:.4f}")

    if roc_auc >= 0.9:
        print("  → Outstanding performance! Model has excellent discriminative ability.")
    elif roc_auc >= 0.8:
        print("  → Excellent performance! Model distinguishes classes very well.")
    elif roc_auc >= 0.7:
        print("  → Good performance! Model has acceptable discriminative ability.")
    else:
        print("  → Fair performance. Consider further model improvement.")
else:
    print("\n(ROC curve not available for this model type)")


# 11.3: Feature Importance (if applicable)
print("\n[11.3] Feature Importance Analysis")
print("-" * 80)
print("""
WHAT IS FEATURE IMPORTANCE?
- Measures how much each feature contributes to predictions
- Helps understand which factors drive loan payback predictions
- Identifies most influential features for business insights

WHY IT MATTERS:
- Provides interpretability (why model makes certain predictions)
- Guides feature engineering (focus on important features)
- Helps identify potential biases or unexpected patterns
- Supports business decision-making and policy
""")

if hasattr(best_model, 'feature_importances_'):
    # For tree-based models (Decision Tree, Random Forest)
    feature_importance = best_model.feature_importances_
    feature_names = feature_cols

    # Create DataFrame and sort
    importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': feature_importance
    }).sort_values('Importance', ascending=False)

    print("\nTop 10 Most Important Features:")
    print(importance_df.head(10).to_string(index=False))

    # Visualize feature importance
    plt.figure(figsize=(12, 8))
    top_features = importance_df.head(15)
    bars = plt.barh(range(len(top_features)), top_features['Importance'],
                    color='#3498db', edgecolor='black', linewidth=2)

    for i, (bar, value) in enumerate(zip(bars, top_features['Importance'])):
        plt.text(value + 0.001, i, f'{value:.4f}',
                va='center', fontsize=10, fontweight='bold')

    plt.yticks(range(len(top_features)), top_features['Feature'],
               fontsize=12, fontweight='bold')
    plt.xlabel('Importance Score', fontsize=15, fontweight='bold')
    plt.title(f'Top 15 Feature Importances: {best_model_name}',
              fontsize=16, fontweight='bold', pad=20)
    plt.grid(axis='x', alpha=0.3)
    plt.xticks(fontsize=13, fontweight='bold')
    plt.tight_layout()
    plt.savefig('10_feature_importance.png', dpi=300, bbox_inches='tight')
    plt.show()

    print(f"\n Feature importance plot saved as '10_feature_importance.png'")

elif hasattr(best_model, 'coef_'):
    # For linear models (Logistic Regression)
    coefficients = best_model.coef_[0]
    feature_names = feature_cols

    # Create DataFrame and sort by absolute value
    importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Coefficient': coefficients,
        'Abs_Coefficient': np.abs(coefficients)
    }).sort_values('Abs_Coefficient', ascending=False)

    print("\nTop 10 Most Influential Features (by coefficient magnitude):")
    print(importance_df[['Feature', 'Coefficient']].head(10).to_string(index=False))

    # Visualize coefficients
    plt.figure(figsize=(12, 8))
    top_features = importance_df.head(15)
    colors = ['#2ecc71' if c > 0 else '#e74c3c' for c in top_features['Coefficient']]
    bars = plt.barh(range(len(top_features)), top_features['Coefficient'],
                    color=colors, edgecolor='black', linewidth=2)

    for i, (bar, value) in enumerate(zip(bars, top_features['Coefficient'])):
        plt.text(value + (0.01 if value > 0 else -0.01), i, f'{value:.4f}',
                va='center', ha='left' if value > 0 else 'right',
                fontsize=10, fontweight='bold')

    plt.yticks(range(len(top_features)), top_features['Feature'],
               fontsize=12, fontweight='bold')
    plt.xlabel('Coefficient Value', fontsize=15, fontweight='bold')
    plt.title(f'Top 15 Feature Coefficients: {best_model_name}',
              fontsize=16, fontweight='bold', pad=20)
    plt.axvline(x=0, color='black', linestyle='-', linewidth=1)
    plt.grid(axis='x', alpha=0.3)
    plt.xticks(fontsize=13, fontweight='bold')
    plt.tight_layout()
    plt.savefig('10_feature_importance.png', dpi=300, bbox_inches='tight')
    plt.show()

    print(f"\n Feature coefficients plot saved as '10_feature_importance.png'")
    print("\nNote: Positive coefficients increase loan payback probability")
    print("      Negative coefficients decrease loan payback probability")

elif hasattr(best_model, 'estimators_'):
    # For ensemble models
    print("\n(Feature importance available from base estimators)")
    print("Analyzing ensemble components...")

    # Try to get feature importances from underlying estimators
    try:
        importances = []
        for estimator in best_model.estimators_:
            if hasattr(estimator, 'feature_importances_'):
                importances.append(estimator.feature_importances_)

        if importances:
            avg_importance = np.mean(importances, axis=0)
            importance_df = pd.DataFrame({
                'Feature': feature_cols,
                'Importance': avg_importance
            }).sort_values('Importance', ascending=False)

            print("\nTop 10 Most Important Features (averaged across ensemble):")
            print(importance_df.head(10).to_string(index=False))

            # Visualize
            plt.figure(figsize=(12, 8))
            top_features = importance_df.head(15)
            bars = plt.barh(range(len(top_features)), top_features['Importance'],
                            color='#9b59b6', edgecolor='black', linewidth=2)

            for i, (bar, value) in enumerate(zip(bars, top_features['Importance'])):
                plt.text(value + 0.001, i, f'{value:.4f}',
                        va='center', fontsize=10, fontweight='bold')

            plt.yticks(range(len(top_features)), top_features['Feature'],
                       fontsize=12, fontweight='bold')
            plt.xlabel('Average Importance Score', fontsize=15, fontweight='bold')
            plt.title(f'Top 15 Feature Importances: {best_model_name}',
                      fontsize=16, fontweight='bold', pad=20)
            plt.grid(axis='x', alpha=0.3)
            plt.xticks(fontsize=13, fontweight='bold')
            plt.tight_layout()
            plt.savefig('10_feature_importance.png', dpi=300, bbox_inches='tight')
            plt.show()

            print(f"\n Feature importance plot saved as '10_feature_importance.png'")
    except:
        print("  Unable to extract feature importances from ensemble components")

else:
    print("\n(Feature importance not available for this model type)")


# =========================================================================================
# SECTION 12: GENERATING PREDICTIONS ON TEST SET
# =========================================================================================

"""
OBJECTIVE: Use the best model to generate predictions for the test dataset

WHY THIS STEP?
- This is the ultimate goal: predict loan payback for new borrowers
- Test set represents real-world scenario (no target labels)
- Predictions will be used for business decisions (loan approvals)
- Must generate probability estimates for risk assessment

OUTPUT FORMAT:
- id: Borrower identifier
- loan_paid_back: Predicted probability of loan payback (0 to 1)
"""

print("\n" + "="*80)
print("SECTION 12: GENERATING PREDICTIONS ON TEST SET")
print("="*80)

print(f"\nUsing best model: {best_model_name}")
print(f"Test set size: {len(X_test_scaled)} samples")

# Generate predictions
if hasattr(best_model, 'predict_proba'):
    # Use probability estimates
    test_predictions_proba = best_model.predict_proba(X_test_scaled)[:, 1]
    print("\n Generated probability predictions")
else:
    # Use binary predictions (convert to pseudo-probabilities)
    test_predictions = best_model.predict(X_test_scaled)
    test_predictions_proba = test_predictions.astype(float)
    print("\n Generated binary predictions (converted to probabilities)")

# Create submission DataFrame
submission_df = pd.DataFrame({
    'id': test_df['id'],
    'loan_paid_back': test_predictions_proba
})

print("\nFirst 10 predictions:")
print(submission_df.head(10).to_string(index=False))

print("\nPrediction Statistics:")
print(f"  Mean probability: {test_predictions_proba.mean():.4f}")
print(f"  Median probability: {np.median(test_predictions_proba):.4f}")
print(f"  Std deviation: {test_predictions_proba.std():.4f}")
print(f"  Min probability: {test_predictions_proba.min():.4f}")
print(f"  Max probability: {test_predictions_proba.max():.4f}")

# Visualize prediction distribution
plt.figure(figsize=(12, 6))
plt.hist(test_predictions_proba, bins=50, edgecolor='black',
         color='#3498db', alpha=0.7, linewidth=1.5)
plt.axvline(test_predictions_proba.mean(), color='#e74c3c',
            linestyle='--', linewidth=3, label=f'Mean: {test_predictions_proba.mean():.4f}')
plt.axvline(np.median(test_predictions_proba), color='#2ecc71',
            linestyle='--', linewidth=3, label=f'Median: {np.median(test_predictions_proba):.4f}')
plt.xlabel('Predicted Probability of Loan Payback', fontsize=15, fontweight='bold')
plt.ylabel('Frequency', fontsize=15, fontweight='bold')
plt.title('Distribution of Test Set Predictions', fontsize=16, fontweight='bold', pad=20)
plt.legend(fontsize=13, loc='best')
plt.grid(alpha=0.3)
plt.xticks(fontsize=13, fontweight='bold')
plt.yticks(fontsize=13, fontweight='bold')
plt.tight_layout()
plt.savefig('11_test_predictions_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"\n Prediction distribution plot saved as '11_test_predictions_distribution.png'")

# Save predictions to CSV
submission_df.to_csv('loan_predictions.csv', index=False)
print(f"\n Predictions saved to 'loan_predictions.csv'")

print("""
INTERPRETATION OF PREDICTIONS:
-------------------------------
- Probabilities close to 1.0: High confidence loan will be paid back
- Probabilities close to 0.0: High confidence loan will NOT be paid back
- Probabilities around 0.5: Uncertain predictions (borderline cases)

BUSINESS APPLICATION:
---------------------
- Set a threshold based on risk tolerance (e.g., 0.7)
- Probabilities > threshold: Approve loan
- Probabilities < threshold: Reject loan or require additional review
- Adjust threshold to balance approval rate vs default risk
""")